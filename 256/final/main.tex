\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{indentfirst}
\renewcommand{\baselinestretch}{1}
\usepackage{subcaption}
\usepackage{float}
\usepackage{dsfont} %change indicator function
\graphicspath{ {images/} }
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{fullpage} % changes the margin

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
{\Large\textbf{Final Exam} \hfill \\
AMS 256 \hfill Spring 2018 \\
Wyara Vanesa Moura e Silva \hfill June 10\\}

\section*{Question 1}

Consider the linear regression model
\begin{equation*}
\begin{array}{lclllll}
y_i & = &  \beta_1x_{1i} + \beta_2x_{2i} + \epsilon_i, \hspace{0.5cm}  \epsilon_i \stackrel{i.i.d.}{\sim} N(0,\sigma^2) \\ \\ 

\textbf{y} & = &  [82, 79, 74, 83, 80, 81, 84, 81]^T\\
\textbf{x}_1 & = & [10, 9, 9, 11, 11, 10, 10, 12]^T \\
\textbf{x}_2 & = & [15, 14, 13, 15, 14, 14, 16, 13]^T \\
\end{array}
\end{equation*}

See the \texttt{R} code used to do the simple algebra in Appendix \ref{code}.
\subsection*{(a)} Provide the least square estimates of $\beta_1$, $\beta_2$ and $\sigma^2$.

\noindent
\textit{Solution:}\\

Then, by definition we have that the least square estimate are given by: $\hat{\boldsymbol{\beta}}$ = $(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$. Thus, we can get:
\begin{equation*}
\textbf{X}^T = 
\begin{pmatrix} 10 & 9 & 9 & 11 & 11 &10 & 10 & 12 \\ 15 & 14 & 13 & 15 & 14 & 14 & 16 & 13 \end{pmatrix}^T 
\end{equation*}

\begin{equation*}
\textbf{X}^T \textbf{X} = 
\begin{pmatrix} 848 & 1168 \\ 1168 & 1632 \end{pmatrix}
\end{equation*}

\begin{equation*}
(\textbf{X}^T \textbf{X})^{-1} = \dfrac{1}{19721}
\begin{pmatrix} 1632 & -1168 \\ -1168 & 848 \end{pmatrix} =
\begin{pmatrix} 0.08279 & -0.05925 \\ -0.05925 & 0.043019 \end{pmatrix} 
\end{equation*}

\begin{equation*}
\textbf{X}^T \textbf{y} = 
\begin{pmatrix} 6612 \\ 9194 \end{pmatrix} 
\end{equation*}

Therefore,
\begin{equation*}
\hat{\boldsymbol{\beta}} = (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y} = 
\begin{pmatrix} 2.647727 \\ 3.738636 \end{pmatrix} 
\end{equation*}

Now, for the mean squared error (MSE), we know that:
\begin{equation*}
\begin{array}{lclllll}
\hat{\sigma}^2 & = & \dfrac{\sum_{i=1}^{n}(y - \textbf{X}\hat{\boldsymbol{\beta}})^2}{n-\mbox{rank}(\textbf{X})}
\end{array}
\end{equation*}
\noindent
In which rank(\textbf{X}) = 2. Then,
\begin{equation*}
\begin{array}{lclllll}
(y - \textbf{X}\hat{\boldsymbol{\beta}})^2 & = & [0.31004, 8.00632, 2.45919, 4.86002, 2.14888, 4.76033, 5.26911, 0.39062]^T
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{lclllll}
\hat{\sigma}^2 & = & \dfrac{28.20455}{8-2} & = & 4.70075
\end{array}
\end{equation*}

\subsection*{(b)} Provide 95\% confidence intervals for $\beta_1$ and $2*\beta_1$ + $\beta_2$.

\noindent
\textit{Solution:}\\

In order to get the confidence intervals, we can use the t-statistic given by:
\begin{equation*}
\begin{array}{lclllll}
t & = & \dfrac{ \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} -  \boldsymbol{\lambda}^T\boldsymbol{\beta}}{\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}}} \sim t_{n-\mbox{rank}(\textbf{X})}
\end{array}
\end{equation*}

\begin{itemize}
\item First, $\beta_1$. 
\end{itemize}

In this case, $\boldsymbol{\lambda}^T = [1, 0]^T$, and all the other estimates and statistics we already have from (a). Thus,
\begin{equation*}
\begin{array}{lclllll}
P\left( \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} - t_{\frac{\alpha}{2}, n-2} \hspace{0.2cm} \displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} \leq \boldsymbol{\lambda}^T\boldsymbol{\beta} \leq \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} + t_{\frac{\alpha}{2}, n-2} \hspace{0.2cm} \displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} \right) = 1 - \alpha
\end{array}
\end{equation*}

So, as we want a 95\% confidence interval, the $ t_{\frac{\alpha}{2}, n-2}$ = 2.44691.
\begin{equation*}
\begin{array}{lclllll}
\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} & = & 0.62384
\end{array}
\end{equation*}

Then, we can find the interval, which is given by:
\begin{equation*}
\begin{array}{lclllll}
P\left( 2.64772 - t_{\frac{\alpha}{2}, n-2} * \hspace{0.2cm} 0.62384 \leq \boldsymbol{\lambda}^T\boldsymbol{\beta} \leq 2.64772 + t_{\frac{\alpha}{2}, n-2} * \hspace{0.2cm} 0.62384 \right) = 0.95 \\ \\
\end{array}
\end{equation*}

Therefore, $\left[ 1.12122, 4.17422 \right]$ is the 95\% confidence interval for $\beta_1$.

\begin{itemize}
\item Second, for $2*\beta_1 + \beta_2$.
\end{itemize}

In this case, $\boldsymbol{\lambda}^T = [2, 1]^T$, and all the other estimates and statistics we already have from (a). Thus,
\begin{equation*}
\begin{array}{lclllll}
P\left( \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} - t_{\frac{\alpha}{2}, n-2} \hspace{0.2cm} \displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} \leq \boldsymbol{\lambda}^T\boldsymbol{\beta} \leq \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} + t_{\frac{\alpha}{2}, n-2} \hspace{0.2cm} \displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} \right) = 1 - \alpha
\end{array}
\end{equation*}

So, as we want a 95\% confidence interval, the $ t_{\frac{\alpha}{2}, n-2}$ = 2.44691.
\begin{equation*}
\begin{array}{lclllll}
\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} & = & 0.80301
\end{array}
\end{equation*}

Then, we can find the interval, which is given by:
\begin{equation*}
\begin{array}{lclllll}
P\left( 9.03409 - t_{\frac{\alpha}{2}, n-2} * \hspace{0.2cm} 0.80301 \leq \boldsymbol{\lambda}^T\boldsymbol{\beta} \leq 9.03409 + t_{\frac{\alpha}{2}, n-2} * \hspace{0.2cm} 0.80301 \right) = 0.95 \\ \\
\end{array}
\end{equation*}

Therefore, $\left[ 7.06919, 10.99899 \right]$ is the 95\% confidence interval for $2*\beta_1 + \beta_2$.

\subsection*{(c)} 
Perform a $\alpha$ = 0.01 level test for H$_0$: $\beta_2$ = 3.

\noindent
\textit{Solution:}\\

In order, to perform the test, first we know that $\boldsymbol{\lambda}^T = [0,1]$, and then we can do the test using the t-statistic given by:
\begin{equation*}
\begin{array}{lclllll}
t & = & \dfrac{ \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} -  \boldsymbol{\lambda}^T\boldsymbol{\beta}}{\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}}} \sim t_{n-\mbox{rank}(\textbf{X})}
\end{array}
\end{equation*}

In this case, $\boldsymbol{\lambda}^T\boldsymbol{\beta} = 3$, then using the estimate and statistics from (a),
\begin{equation*}
\begin{array}{lclllll}
\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} & = & 0.44969
\end{array}
\end{equation*}

Then, the t-statistic and p-value are given by:
\begin{equation*}
\begin{array}{lclllll}
t_{obs} & = & \dfrac{3.73863 - 3}{0.44969}&  = & 1.64253 \\\\

p-\mbox{value} & = & P_{H_0}(|t|>t_{obs}) = P_{H_0}(|t|>1.64253) & = & 0.15158
\end{array}
\end{equation*}

Therefore, we fail to reject the null hypothesis, as $p$-value $>$ 0.1.

\subsection*{(d)} 
Find $p$-value for the test H$_0$: $\beta_1$ = $\beta_2$.

\noindent
\textit{Solution:}\\

In order, to perform the test, first we know that $\boldsymbol{\lambda}^T = [1,-1]$, and then we can do the test using the t-statistic given by:
\begin{equation*}
\begin{array}{lclllll}
t & = & \dfrac{ \boldsymbol{\lambda}^T\boldsymbol{\hat{\beta}} -  \boldsymbol{\lambda}^T\boldsymbol{\beta}}{\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}}} \sim t_{n-\mbox{rank}(\textbf{X})}
\end{array}
\end{equation*}

In this case, $\boldsymbol{\lambda}^T\boldsymbol{\beta} = 0$, then using the estimate and statistics from (a),
\begin{equation*}
\begin{array}{lclllll}
\displaystyle\sqrt[]{\hat{\sigma}^2 \boldsymbol{\lambda}^T (\textbf{X}^T \textbf{X})^{-1} \boldsymbol{\lambda}} & = & 1.07167
\end{array}
\end{equation*}

Then, the t-statistic and p-value are given by:
\begin{equation*}
\begin{array}{lclllll}
t_{obs} & = & \dfrac{-1.09091}{ 1.07167}&  = & -1.01795 \\\\

p-\mbox{value} & = & P_{H_0}(|t|>t_{obs}) = P_{H_0}(|t|>-1.01795) & = & 0.34797
\end{array}
\end{equation*}

Therefore, we fail to reject the null hypothesis, as $p$-value $>$ 0.1.

\section*{Question 2}

Consider the setting and the dataset in the previous question. Use the \texttt{R} package to
run linear regression. Provide

\subsection*{(a)} $p$-value for testing $\beta_2$ = 0. 

\noindent
\textit{Solution:}\\

Then, for the \texttt{lm} function in \texttt{R}, we get as result from the output:
\begin{verbatim}
> fit.2 = lm(y ~ -1 + x1 + x2)
> summary(fit.2)

Call:
lm(formula = y ~ -1 + x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.29545 -1.65057  0.03409  1.72159  2.82955 

Coefficients:
   Estimate Std. Error t value Pr(>|t|)    
x1   2.6477     0.6238   4.244 0.005415 ** 
x2   3.7386     0.4497   8.314 0.000164 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.168 on 6 degrees of freedom
Multiple R-squared:  0.9995,	Adjusted R-squared:  0.9993 
F-statistic:  5518 on 2 and 6 DF,  p-value: 1.604e-10

\end{verbatim}

In which we can check that the $p$-value for testing $\beta_2$ = 0 is equal to 0.000164, in which we have strong evidence to reject the hypothesis that $\beta_2$ = 0, using a $\alpha$ = 0.001.

\subsection*{(b)} Draw the joint confidence set for ($\beta_1$, $\beta_2$). 

\noindent
\textit{Solution:}\\

In order to get the joint confidence interval, we are goint to use the \texttt{ellipse} function in \texttt{R}, This function happens to have a specialized method for \texttt{lm} models. then, we get as output:

\begin{verbatim}
> library(ellipse)

> plot(ellipse(fit.2, which = c(1,2), level = 0.95), type = 'l', 
+ cex.lab=1.5,cex.main = 2,cex.axis=1.8)

> points(fit.2$coefficients[1], fit.2$coefficients[2], col=2)
> segments(fit.2$coefficients[1],0,fit.2$coefficients[1],fit.2$coefficients[2])
> segments(0,fit.2$coefficients[2],fit.2$coefficients[1],fit.2$coefficients[2])
> legend(3.5,4.5, pch=1, box.lty = 0,cex=1.6, 
+ legend=c(expression(paste(hat(beta)[1]:hat(beta)[2]))),bty='n',col=c(2))

\end{verbatim}

\begin{figure}[H]
\centering
\includegraphics[width = 0.49\textwidth, height =0.27\textheight]{joint.pdf}
\caption{Joint confidence set of ($\beta_1$, $\beta_2$).}
\label{Joint}
\end{figure}

Thus, Figure \ref{Joint} display the 95\% contour of the joint distribution of $\hat{\beta}_1$ and $\hat{\beta}_2$. So, we have 95\% of the probability mass of the joint distribution falls within this ellipse.

\subsection*{(c)} Add an intercept to the model and check if predictor coefficients are significant. 

\noindent
\textit{Solution:}\\

Then, as output from the \texttt{lm} function in \texttt{R}, we get as result:

\begin{verbatim}
> summary(fit.3)

Call:
lm(formula = y ~ x1 + x2)

Residuals:
     1      2      3      4      5      6      7      8 
 0.125  1.125 -1.500 -0.500 -1.125  1.500 -0.250  0.625 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  30.0000     8.2583   3.633  0.01502 * 
x1            1.6250     0.4556   3.567  0.01610 * 
x2            2.3750     0.4556   5.213  0.00343 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.245 on 5 degrees of freedom
Multiple R-squared:  0.8826,	Adjusted R-squared:  0.8356 
F-statistic: 18.79 on 2 and 5 DF,  p-value: 0.004725
 
\end{verbatim}

Therefore, the predictors coefficients are significant, as the $p$-value are less than 0.01 for all three  coefficents (including the intercept), it can be seen in column named \texttt{Pr(>|t|)} in the output given by the \texttt{lm} function above.

\section*{Question 3}

Considering a model: \textbf{y} = \textbf{X}$\boldsymbol{\beta}$ + $\boldsymbol{\epsilon}$, $\boldsymbol{\epsilon}$ $\sim$ $N(\textbf{0}, \sigma^2$\textbf{I}), where $\textbf{X}$ = [ \textbf{1} : \textbf{x}$_1$ : \textbf{x}$_2$ : ... : \textbf{x}$_p]$. Show that the model fitting statistic $R^2$ for this model is simply the square of the correlations between observed and predicted values of $y$.

\noindent
\textit{Solution:}\\

The observed values are: $\textbf{y}$ = [$y_i$, ..., $y_n$]$^T$ and the predicted values are: $\hat{\textbf{y}}$ = [$\hat{y}_i$, ..., $\hat{y}_n$]$^T$. Then, by definition we know that:
\begin{equation*}
\begin{array}{lclllll}
\mbox{corr}(\textbf{y}, \hat{\textbf{y}}) & = & \dfrac{\mbox{cov}(\textbf{y}, \hat{\textbf{y}})}{\displaystyle\sqrt[]{\mbox{Var}(\textbf{y})\mbox{Var}(\hat{\textbf{y}})}} & = & \dfrac{\mbox{S}_{\textbf{y},\hat{\textbf{y}}}}{\displaystyle\sqrt[]{\mbox{S}_{\textbf{y}} \mbox{S}_{\hat{\textbf{y}}}}}
\end{array}
\end{equation*}

Also, by definition, we have that:
\begin{equation*}
\begin{array}{lclllll}
\mbox{S}_{\textbf{y}} & = & \dfrac{1}{n} \displaystyle\sum_{i=1}^{n}(y_i - \bar{y})^2 & = & \dfrac{1}{n} \textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y} \\

\mbox{S}_{\hat{\textbf{y}}} & = & \dfrac{1}{n} \displaystyle\sum_{i=1}^{n}(\hat{y}_i - \bar{\hat{y}})^2 & = & \dfrac{1}{n} \hat{\textbf{y}}^T(\textbf{I} - \textbf{P}_1)\hat{\textbf{y}} \\

\mbox{S}_{\textbf{y},\hat{\textbf{y}}} & = & \dfrac{1}{n} \displaystyle\sum_{i=1}^{n}(y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}}) & = & \dfrac{1}{n} \textbf{y}^T(\textbf{I} - \textbf{P}_1)\hat{\textbf{y}} \\
\end{array}
\end{equation*}

In which $\textbf{P}_1$ is the perpendicular projection matrix onto $\mathcal{C}(\textbf{W})$, \textbf{W} = [1, ..., 1]$^T$. In addition, we know that $\textbf{P}_x$ is the perpendicular projection matrix onto $\mathcal{C}(\textbf{X})$. Another information would be, $\hat{\textbf{y}} = \textbf{P}_x\textbf{y}$. Therefore,

\begin{proof}
\begin{equation*}
\begin{array}{lclllll}
\mbox{corr}(\textbf{y}, \hat{\textbf{y}})^2 & = & \dfrac{\mbox{S}_{\textbf{y},\hat{\textbf{y}}}^2}{\mbox{S}_{\textbf{y}} \mbox{S}_{\hat{\textbf{y}}}} & = & \dfrac{[ \textbf{y}^T(\textbf{I} - \textbf{P}_1)\hat{\textbf{y}}]^2}{[\textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y}][\hat{\textbf{y}}^T(\textbf{I} - \textbf{P}_1)\hat{\textbf{y}}]} \\\\

& & & = & \dfrac{[ \textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{P}_x\textbf{y}]^2}{[\textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y}][\textbf{y}^T\textbf{P}_x(\textbf{I} - \textbf{P}_1)\textbf{P}_x\textbf{y}]} \\\\

& & & = & \dfrac{[ \textbf{y}^T\textbf{P}_x\textbf{y} - \textbf{y}^T\textbf{P}_1\textbf{y}]^2}{[\textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y}] [\textbf{y}^T\textbf{P}_x\textbf{y} - \textbf{y}^T\textbf{P}_1\textbf{y}]} \\ \\


& & & = & \dfrac{[ \textbf{y}^T\textbf{P}_x\textbf{y} - \textbf{y}^T\textbf{P}_1\textbf{y}]}{[\textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y}]}\\\\

& = & \dfrac{\textbf{y}^T(\textbf{P}_x - \textbf{P}_1)\textbf{y}}{\textbf{y}^T(\textbf{I} - \textbf{P}_1)\textbf{y}} = R^2

\end{array}
\end{equation*}

\end{proof}
\noindent 
This ratio correspond to the regression sum of squared by total sum of squared, in which correspond to the definition of the coefficient of determination $R^2$, ending the proof.

\section*{Question 3}

Christensen presents mathematics ineptitude scores (Score $y_{ijk}$) for a group of $N=35$
students categorized by:

\begin{itemize}
\item Major $i$ (1 = Economics, 2 = Anthropology, and 3 = Sociology);
\item High school background ("BG") $j$ (1 = Rural and 2 = Urban).
\end{itemize}

The model is a two-way ANOVA, being:
\begin{equation*}
\begin{array}{lclllll}
y_{ijk} & = & \mu + \alpha_i + \nu_j + \gamma_{ij} + \epsilon_{ijk}
\end{array}
\end{equation*}
The constraints used were: $\alpha_1 = \nu_1 = 0$. Also $\gamma_{ij} = 0$ if $i=1$ and $j=1$.

See the \texttt{R} code used to do the analysis in Appendix \ref{code}, as well as the data used.

\subsubsection*{(a)} 
Which group of students has the lowest average score? (What is it?) Which group
of students has the highest average score? (What is it?)

\noindent
\textit{Solution:}\\

We can see the graph of means of scores by the groups, in Figure \ref{int} that the group lowest average score is given by students from Economics major with BG Rural, and the highest average score is students from Anthropology major with BG Rural.
\begin{figure}[H]
\centering
\includegraphics[width = 0.49\textwidth, height =0.27\textheight]{interact.pdf}
\includegraphics[width = 0.49\textwidth, height =0.27\textheight]{interact-2.pdf}
\caption{Mean Scores of the students by interaction between the groups.}
\label{int}
\end{figure}

In which, we can get from the output of the \texttt{lm} function in \texttt{R}, that: The highest average score gets value: $\mu$ + Major $2$ = 0.9122 + 1.9418 =  2.8540. The lowest average score gets value: $\mu$ = 0.9122 = 0.9122. We have this value because the constraints in \texttt{R} is Major $1$ = BG $1$ = 0.

This means, that we expect to have the higher scores from students that have major in Anthropology and their high school background was Rural. And a lower scores from students hat have major in Economics and their high school background was Rural.

\subsubsection*{(b)} 
In the \texttt{summary(.)} output there is an F-statistic, F = $2.553$ with 5 and 29 degrees of freedom.

\noindent
\textit{Solution:}\\

\begin{enumerate}
\item (i) What are the null and alternative hypotheses being tested?


From default the F-test tests from the \texttt{summary(.)} function in \texttt{R}, test the reduced model with $y_{ijk}$ = $\mu$ versus the full model. We can check that we get the same result if we fit two models, one with $y_{ijk}$ = $\mu$ and the other with all the groups, and then use the \texttt{anova} function to compare the two models.

\item (ii) What conclusion would you make? (Please state in general terms that relate to the
groups rather than parameters).

We can check the output from the \texttt{summary(.)} function in \texttt{R}, that the $p$-value from the F-test is given by 0.0533, thus we reject the null hypothesis of the reduced model using $\alpha = 0.1$. Then, we have low evidence, but there is still evidence that there are differences between groups. And in Figure \ref{int} we can observe a little of this behavior.

And there is low evidence of interaction between Economics and Sociology, whether the student with BG Rural or Urban. In Figure \ref{int}, we can note that the effect in the mean scores of BG being Rural, are always higher than Urban, if evaluate the majors Anthropology and Sociology. As well as the mean of scores of majors Anthropology are always higher than Sociology if we evaluated them separated. Moreover, we have an interaction between Economics and Sociology. As we point out in the letter (a) we can expect a real change in the mean scores, of a student with BG rural or urban, depending on whether the major is Economics or Sociology.
\end{enumerate}


\newpage
\section{Appendix}\label{code}

See below all the R Code used to perform the analysis.

\begin{verbatim}

rm(list=ls(all=TRUE))
set.seed(27)

# take home - final - AMS 206

y = c(82, 79, 74, 83, 80, 81, 84, 81)
x1 = c(10, 9, 9, 11, 11, 10, 10, 12)
x2 = c(15, 14, 13, 15, 14, 14, 16, 13)

X = cbind(x1, x2)

fit.1 = lm(y ~ -1 + x1 + x2) 
summary(fit.1)
p = 2
n = length(y)
XtX = t(X)%*%X
invXtX = solve(XtX)
Xty = t(X)%*%y

# Question 1
# 1.a
beta.hat = solve(t(X)%*%X)%*%t(X)%*%y
sigma.hat = sum((y-X%*%beta.hat)^2)/(n-p)

# 1.b
lambda.1 = c(1,0)
var.est.1 = sqrt((sigma.hat)%*%(t(lambda.1)%*%invXtX%*%lambda.1))

t(lambda.1)%*%beta.hat + qt(0.975,6)*var.est.1
t(lambda.1)%*%beta.hat - qt(0.975,6)*var.est.1

lambda.2 = c(2,1)
var.est.2 = sqrt((sigma.hat)%*%(t(lambda.2)%*%invXtX%*%lambda.2))

t(lambda.2)%*%beta.hat + qt(0.975,6)*var.est.2
t(lambda.2)%*%beta.hat - qt(0.975,6)*var.est.2

# 1.c
lambda.3 = c(0,1)
var.est.3 = sqrt((sigma.hat)%*%(t(lambda.3)%*%invXtX%*%lambda.3))

t(lambda.3)%*%beta.hat + qt(0.975,6)*var.est.3
t(lambda.3)%*%beta.hat - qt(0.975,6)*var.est.3

t.val.3 = (t(lambda.3)%*%beta.hat - 3) / var.est.3
pval.3 = (1-pt(abs(t.val.3),n-p))*2

# 1.d
lambda.4 = c(1,-1)
var.est.4 = sqrt((sigma.hat)%*%(t(lambda.4)%*%invXtX%*%lambda.4))

t(lambda.4)%*%beta.hat + qt(0.975,6)*var.est.4
t(lambda.4)%*%beta.hat - qt(0.975,6)*var.est.4

t.val.4 = (t(lambda.4)%*%beta.hat - 0) / var.est.4
pval.4 = (1-pt(abs(t.val.4),n-p))*2

# Question 2
# 2.a
fit.2 = lm(y ~ -1 + x1 + x2)
summary(fit.2)

lambda.5 = c(0,1)
var.est.5 = sqrt((sigma.hat^2)%*%(t(lambda.5)%*%invXtX%*%lambda.5))

t.val.5 = (t(lambda.5)%*%beta.hat - 0) / var.est.5
pval.5 = (1-pt(abs(t.val.5),n-p))*2

# 2.b
library(ellipse)
plot(ellipse(fit.2, which = c(1,2), level = 0.95), type = 'l', 
cex.lab=1.5,cex.main = 2,cex.axis=1.8)
points(fit.2$coefficients[1], fit.2$coefficients[2], col=2)
segments(fit.2$coefficients[1],0,fit.2$coefficients[1],fit.2$coefficients[2])
segments(0,fit.2$coefficients[2],fit.2$coefficients[1],fit.2$coefficients[2])
legend(3.5,4.5, pch=1, box.lty = 0,cex=1.6, 
legend=c(expression(paste(hat(beta)[1]:hat(beta)[2]))),bty='n',col=c(2))

# 2.c
fit.3 = lm(y ~ x1 + x2)
summary(fit.3)

# Question 4
data = matrix(c(0.620, 1, 1,
  1.342, 1, 1,
  0.669, 1, 1,
  0.687, 1, 1,
  0.155, 1, 1,
  2.000, 1, 1,
  1.228, 1, 2,
  3.762, 1, 2,
  2.219, 1, 2,
  4.207, 1, 2,
  0.615, 1, 3,
  2.245, 1, 3,
  2.077, 1, 3,
  3.357, 1, 3,
  1.182, 2, 1,
  1.068, 2, 1,
  2.545, 2, 1,
  2.233, 2, 1, 
  2.664, 2, 1, 
  1.002, 2, 1, 
  2.506, 2, 1,
  4.285, 2, 1,
  1.696, 2, 1,
  3.080, 2, 2, 
  2.741, 2, 2,
  2.522, 2, 2,
  1.647, 2, 2,
  1.999, 2, 2,
  2.939, 2, 2,
  2.240, 2, 3,
  0.330, 2, 3,
  3.453, 2, 3,
  1.527, 2, 3,
  0.809, 2, 3,
  1.942, 2, 3), nrow = 35, ncol = 3, byrow = TRUE,
  dimnames=list(c(seq(1,35,1)),c("Scores","BG","Major")))
data = as.data.frame(data)  

fit.4 = lm(Scores ~ as.factor(Major)*as.factor(BG), data=data)
summary(fit.4)
anova(fit.4)

names <- c('Major','BG')
data[, names] = lapply(data[ , names], factor)
levels(data$Major)[c(1,2,3)] = c('Economics', 'Anthropology', "Sociology")
levels(data$BG)[c(1,2)] = c("Rural", "Urban")

interaction.plot(data$Major, data$BG, data$Scores, trace.lab="BG",
legend=F,cex.axis=1.5, cex.lab=1.5, xlab="Major", ylab="Mean of Score", cex=1.5)
legend("bottomright", lty = c(2,1), box.lty = 0,cex=1.5, legend=c("Rural","Urban"))

interaction.plot(data$BG, data$Major, data$Scores, trace.lab="Major",cex.axis=1.5, 
cex.lab=1.5, xlab="BG", ylab="Mean of Score", cex=1.5)

fit.5 = lm(Scores ~ 1, data=data)

anova(fit.4, fit.5)

\end{verbatim}


\end{document}
